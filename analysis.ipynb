{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import easydict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from maml.utils import load_dataset, load_model, update_parameters\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# log graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_logs(log_type, logs, ax):\n",
    "    if log_type == 'error':\n",
    "        train_log = np.array(logs['train_error'])\n",
    "        valid_log = np.array(logs['valid_error'])\n",
    "        test_log = np.array(logs['test_error'])\n",
    "        ymin = 0.0\n",
    "        ymax = 3.0\n",
    "        ax.set_title('error graph, test error: {}'.format(round(test_log[-1],4)))\n",
    "    elif log_type == 'accuracy':\n",
    "        train_log = np.array(logs['train_accuracy'])\n",
    "        valid_log = np.array(logs['valid_accuracy'])\n",
    "        test_log = np.array(logs['test_accuracy'])\n",
    "        ymin = 0.0\n",
    "        ymax = 1.0\n",
    "        ax.set_title('accruacy graph, test accruacy: {}'.format(round(test_log[-1],4)))\n",
    "    \n",
    "    ax.plot(train_log.nonzero()[0], train_log[train_log.nonzero()[0]], label='train')\n",
    "    ax.plot(valid_log.nonzero()[0], valid_log[valid_log.nonzero()[0]], label='valid')\n",
    "    ax.scatter(test_log.nonzero()[0], test_log[test_log.nonzero()[0]], s=100, color='red', label='test')\n",
    "    \n",
    "    ax.set_ylim([ymin, ymax])\n",
    "    ax.legend()\n",
    "\n",
    "def plot_diff_logs(filename_list, mode='valid'):\n",
    "    fig, axes = plt.subplots(1, 2, sharey=False, figsize=(16,8))\n",
    "    \n",
    "    error_logs_list = []\n",
    "    accuracy_logs_list = []\n",
    "    \n",
    "    error = mode + '_error'\n",
    "    accuracy = mode + '_accuracy'\n",
    "    \n",
    "    for filename in filename_list:\n",
    "        file_logs = pd.read_csv(filename)\n",
    "        error_logs_list.append(np.array(file_logs[error]))\n",
    "        accuracy_logs_list.append(np.array(file_logs[accuracy]))\n",
    "    \n",
    "    for filename, error_logs in list(zip(filename_list, error_logs_list)):\n",
    "        axes[0].plot(error_logs.nonzero()[0], error_logs[error_logs.nonzero()[0]], label=\"_\".join(filename.split(\"/\")[2].split(\"_\")[1:]))\n",
    "    axes[0].set_ylim([0.0, 3.0])\n",
    "    axes[0].legend()\n",
    "\n",
    "    for filename, accuracy_logs in list(zip(filename_list, accuracy_logs_list)):\n",
    "        axes[1].plot(accuracy_logs.nonzero()[0], accuracy_logs[accuracy_logs.nonzero()[0]], label=\"_\".join(filename.split(\"/\")[2].split(\"_\")[1:]))\n",
    "    axes[1].set_ylim([0.0, 1.0])\n",
    "    axes[1].legend()\n",
    "        \n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './output/'\n",
    "file_list = os.listdir(path)\n",
    "file_list = sorted([f for f in file_list if 'miniimagenet' in f])\n",
    "print (file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = [f for f in file_list if 'both_inner' in f]\n",
    "filename = './output/{}/logs/logs.csv'.format(file[0])\n",
    "logs = pd.read_csv(filename)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, sharey=False, figsize=(16, 8))\n",
    "\n",
    "plot_logs(log_type='error', logs=logs, ax=axes[0])\n",
    "plot_logs(log_type='accuracy', logs=logs, ax=axes[1])\n",
    "\n",
    "plt.suptitle(filename.split(\"/\")[2])\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file1 = [f for f in file_list if 'both_inner' in f]\n",
    "file2 = [f for f in file_list if 'classifier_inner' in f]\n",
    "file3 = [f for f in file_list if 'extractor_inner' in f]\n",
    "\n",
    "filename1 = './output/{}/logs/logs.csv'.format(file1[0])\n",
    "filename2 = './output/{}/logs/logs.csv'.format(file2[0])\n",
    "filename3 = './output/{}/logs/logs.csv'.format(file3[0])\n",
    "\n",
    "filename_list = [filename1, filename2, filename3]\n",
    "plot_diff_logs(filename_list, mode='valid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overfitting test\n",
    "### Overfitting 1: meta train <-> meta test\n",
    "### Overfitting 2: support <-> query in meta test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sample_task(dataset):\n",
    "    sample_task = dataset.sample_task()\n",
    "    for idx, (image, label) in enumerate(sample_task['train']):\n",
    "        if idx == 0:\n",
    "            s_images = image.unsqueeze(0)\n",
    "            s_labels = [label]\n",
    "            s_real_labels = [sample_task['train'].index[label]]\n",
    "        else:\n",
    "            s_images = torch.cat([s_images, image.unsqueeze(0)], dim=0)\n",
    "            s_labels.append(label)\n",
    "            s_real_labels.append(sample_task['train'].index[label])\n",
    "    \n",
    "    for idx, (image, label) in enumerate(sample_task['test']):\n",
    "        if idx == 0:\n",
    "            q_images = image.unsqueeze(0)\n",
    "            q_labels = [label]\n",
    "            q_real_labels = [sample_task['test'].index[label]]\n",
    "        else:\n",
    "            q_images = torch.cat([q_images, image.unsqueeze(0)], dim=0)\n",
    "            q_labels.append(label)\n",
    "            q_real_labels.append(sample_task['test'].index[label])\n",
    "    \n",
    "    s_labels = torch.tensor(s_labels).type(torch.LongTensor)\n",
    "    s_real_labels = torch.tensor(s_real_labels).type(torch.LongTensor)\n",
    "    q_labels = torch.tensor(q_labels).type(torch.LongTensor)\n",
    "    q_real_labels = torch.tensor(q_real_labels).type(torch.LongTensor)\n",
    "    return [s_images, s_labels, s_real_labels, q_images, q_labels, q_real_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_number = 1000\n",
    "dataset_args = easydict.EasyDict({'folder': './dataset',\n",
    "                                  'dataset': 'miniimagenet',\n",
    "                                  'num_ways': 5,\n",
    "                                  'num_shots': 5,\n",
    "                                  'download': False})\n",
    "train_tasks = [make_sample_task(load_dataset(dataset_args, 'meta_train')) for _ in tqdm(range(sample_number))]\n",
    "test_tasks = [make_sample_task(load_dataset(dataset_args, 'meta_test')) for _ in tqdm(range(sample_number))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isfloat(value):\n",
    "    try:\n",
    "        float(value)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "def get_arguments(dataset, save_name):\n",
    "    filename = './output/'+'{}_{}/logs/arguments.txt'.format(dataset, save_name)\n",
    "\n",
    "    args = easydict.EasyDict()\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            key, val = line.split(\": \")\n",
    "            if '\\n' in val:\n",
    "                val = val[:-1]\n",
    "            if isfloat(val):\n",
    "                if val.isdigit():\n",
    "                    val = int(val)\n",
    "                else:\n",
    "                    val = float(val)\n",
    "            if val == 'True' or val == 'False':\n",
    "                val = val == 'True'\n",
    "            args[key] = val\n",
    "    return args\n",
    "\n",
    "def print_accuracy(args, sample_tasks, meta_mode, inner_update_number, criterion=None):\n",
    "    device = torch.device(args.device)\n",
    "    sample_number = len(sample_tasks)\n",
    "    \n",
    "    index = ['task{}'.format(str(i+1)) for i in range(sample_number)]\n",
    "    columns = ['Accuracy on support set (before adaptation)']\n",
    "    for i in range(inner_update_number):\n",
    "        columns.append('Accuracy on support set (after {} adaptation)'.format(i+1))\n",
    "        columns.append('Accuracy on query set (after {} adaptation)'.format(i+1))\n",
    "    filename_pd = './output/miniimagenet_{}/logs/{}_results.csv'.format(args.save_name, meta_mode)\n",
    "    test_pd = pd.DataFrame(np.zeros([sample_number, len(columns)]), index=index, columns=columns)\n",
    "    \n",
    "    model = load_model(args)\n",
    "    \n",
    "    filename = './output/miniimagenet_{}/logs/logs.csv'.format(args.save_name)\n",
    "    logs = pd.read_csv(filename)\n",
    "    \n",
    "    if criterion == 'error':\n",
    "        valid_logs = list(logs[logs['valid_error']!=0]['valid_error'])\n",
    "        best_valid_epochs = (valid_logs.index(min(valid_logs))+1)*50\n",
    "    elif criterion == 'accuracy':\n",
    "        valid_logs = list(logs[logs['valid_accuracy']!=0]['valid_accuracy'])\n",
    "        best_valid_epochs = (valid_logs.index(max(valid_logs))+1)*50\n",
    "\n",
    "    checkpoint = args.output_folder + '{}_{}/'.format(args.dataset, args.save_name) + 'models/epochs_{}.pt'.format(best_valid_epochs)\n",
    "    checkpoint = torch.load(checkpoint, map_location=device)\n",
    "    \n",
    "    for idx in tqdm(range(sample_number)):\n",
    "        task_log = []\n",
    "               \n",
    "        model.load_state_dict(checkpoint, strict=True)\n",
    "        model.to(device)\n",
    "\n",
    "        support_input = sample_tasks[idx][0].to(device)\n",
    "        support_target = sample_tasks[idx][1].to(device)\n",
    "        support_real_target = sample_tasks[idx][2]\n",
    "        query_input = sample_tasks[idx][3].to(device)\n",
    "        query_target = sample_tasks[idx][4].to(device)\n",
    "        query_real_target = sample_tasks[idx][5]\n",
    "\n",
    "        model.train()\n",
    "        support_features, support_logit = model(support_input)\n",
    "        _, support_pred_target = torch.max(support_logit, dim=1)\n",
    "        task_log.append((sum(support_target==support_pred_target)/float(len(support_target))).item())\n",
    "        \n",
    "        for number in range(inner_update_number):\n",
    "            inner_loss = F.cross_entropy(support_logit, support_target)\n",
    "            model.zero_grad()\n",
    "            \n",
    "            params = update_parameters(model, inner_loss, extractor_step_size=args.extractor_step_size, classifier_step_size=args.classifier_step_size, first_order=args.first_order)\n",
    "            model.load_state_dict(params, strict=True)\n",
    "            \n",
    "            support_features, support_logit = model(support_input)\n",
    "            _, support_pred_target = torch.max(support_logit, dim=1)\n",
    "            \n",
    "            query_features, query_logit = model(query_input)\n",
    "            _, query_pred_target = torch.max(query_logit, dim=1)\n",
    "                        \n",
    "            task_log.append((sum(support_target==support_pred_target)/float(len(support_target))).item())\n",
    "            task_log.append((sum(query_target==query_pred_target)/float(len(query_target))).item())\n",
    "            \n",
    "        test_pd.iloc[idx] = task_log\n",
    "    test_pd.loc[sample_number+1], test_pd.loc[sample_number+2] = test_pd.mean(axis=0), test_pd.std(axis=0)\n",
    "    test_pd.index = list(test_pd.index[:sample_number]) + ['mean', 'std']\n",
    "    test_pd.to_csv(filename_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset_args.dataset\n",
    "save_name = 'both_inner'\n",
    "\n",
    "args = get_arguments(dataset, save_name)\n",
    "inner_update_number = 1\n",
    "criterion = 'error' # or 'accuracy'\n",
    "\n",
    "print_accuracy(args, train_tasks, meta_mode='meta_train', inner_update_number=inner_update_number, criterion=criterion)\n",
    "print_accuracy(args, test_tasks, meta_mode='meta_test', inner_update_number=inner_update_number, criterion=criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
